{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8072782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install inference-sdk \n",
    "import os \n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1f4ef",
   "metadata": {},
   "source": [
    "Set up inference server and expose model as an API endpoint that our backend can access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14b10232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fastapi\n",
    "# %pip install python-multipart\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from io import BytesIO\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"ROBO_API_KEY\")\n",
    "\n",
    "# Initialize Roboflow client\n",
    "CLIENT = InferenceHTTPClient(\n",
    "    api_url=\"https://detect.roboflow.com\",\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    print(\"Root endpoint was hit\")  # Debugging log\n",
    "    return {\"message\": \"HenSense API is running\"}\n",
    "\n",
    "\n",
    "@app.get(\"/favicon.ico\")\n",
    "async def favicon():\n",
    "    return {}\n",
    "\n",
    "# Function to read image file and preprocess it\n",
    "def read_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3).numpy()\n",
    "    return image\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    # Read file and convert to numpy array\n",
    "    image_data = await file.read()\n",
    "    print(\"Image data type: \", type(image_data))\n",
    "    print(\"Image data: \", image_data)\n",
    "\n",
    "    # Perform inference\n",
    "    result = CLIENT.infer(image_data, model_id=\"healthy-and-sick-chicken-detection-kavqw/18\")\n",
    "\n",
    "    return {\"prediction\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0d78e",
   "metadata": {},
   "source": [
    "Start up the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "04559505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [21881]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [21881]\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()  # allows Uvicorn to run inside Jupyter\n",
    "\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bfeffe",
   "metadata": {},
   "source": [
    "Model fine-tuning using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dd524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
